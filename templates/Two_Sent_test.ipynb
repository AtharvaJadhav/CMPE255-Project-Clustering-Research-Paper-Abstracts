{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Two_Sent_test.ipynb","provenance":[],"collapsed_sections":[]},"file_extension":".py","kernelspec":{"name":"python3","display_name":"Python 3"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"cells":[{"cell_type":"code","metadata":{"id":"8AXqesjGFNZj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1589510700288,"user_tz":-330,"elapsed":37468,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"11fc2a68-5bbb-45ca-969d-626a1cab7d36"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oAm5xPWpEwkg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589510705786,"user_tz":-330,"elapsed":2617,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"27bc11e5-b7e3-45a0-df27-8219255520f7"},"source":["import os\n","os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"_8E2u3vG1xfe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589510719317,"user_tz":-330,"elapsed":12954,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"555bcefc-b2e7-4e86-fa76-841122b4c489"},"source":["! pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n6yRKl7Nx8NV"},"source":["# ***Set parameters***"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RLla__P2wqVL","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1587483329795,"user_tz":-330,"elapsed":1727,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"64d70cc8-2197-4b26-e347-a56f1e819abe"},"source":["#Name of the Clustering algorithm\n","__algo__ = \"KMEANS\"               \n","\n","#Name of the Word Embeddings used (glove, w2v, ftt), MUST set directory below\n","__emb__ = \"ftt\" #@param [\"glove\", \"w2v\", \"ftt\", \"use\"] {allow-input: true}\n","\n","#Name of Sentence Embedding algorithm used\n","__sentemb__ = \"pmeans5\" #@param [\"pmeans5\", \"normalmean\", \"use\"] {allow-input: true}           \n","\n","#Number of records to be read from files\n","recnum =                      2#@param {type: \"number\"} \n","\n","# # #Number of Clusters\n","k = 350\n","\n","# Number of records\n","records = 30000\n","\n","# Size of sentence embedding\n","if __emb__ == \"w2v\":\n","  features = 500\n","if __emb__ == \"glove\":\n","  features = 250\n","if __emb__ == \"ftt\":\n","  features = 1500\n","if __emb__ == \"use\":\n","    features == \"512\"\n","\n","if __sentemb__ == \"normalmean\":\n","  features = int(features/5)\n","\n","#list of megadfs\n","megadfs = []\n","\n","#list of corresponding models\n","models = []\n","\n","# usesqrt = False                   #@param [\"False\", \"True\"] {type:\"raw\", allow-input: true}\n","\n","# #Random Sampling to be True/False for records which are read\n","randomsample = True              #@param [\"False\", \"True\"] {type:\"raw\", allow-input: true} \n","\n","#Directory where embeddings are saved for that selected embedding\n","embedDir = \"./drive/My Drive/ColabOutput/FTXSentEmbs\" #@param [\"./drive/My Drive/ColabOutput/FTXSentEmbs\", \"./drive/My Drive/ColabOutput/W2VSentEmbs\", \"./drive/My Drive/ColabOutput/MegaSentEmbs\"] {allow-input: true}\n","\n","\n","#Directory where models are saved\n","modelDir = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/models/\"           \n","\n","#Directory Where Megadf is to be saved\n","megadfDir = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/MegaDfs/\"         \n","\n","#Directory where plots are saved\n","plotDir = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/plots/\"           \n","\n","#Directory where performance and distribution params are to be stored\n","metadataDir = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/modelMetaData/\" \n","\n","#Directory where test outcomes are saved\n","dumpDir = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/dump/\"              \n","\n","#Model name\n","name = \"{}_{}_{}_K{}_R{}_F{}\".format(__algo__, __emb__, __sentemb__, k, records, features)\n","modelname = \"{}_model.pkl\".format(name)\n","\n","#Model Address\n","modeladd = modelDir+modelname\n","print(modeladd)\n","\n","# ClusterDF name\n","clusterdfname = \"{}_clustered_megadf.pkl\".format(name)\n","\n","# Clusterdf address\n","clusterdfadd = megadfDir+clusterdfname\n","print(clusterdfadd)\n","\n","# Directory where result csv's are stored\n","resultsDir = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/results/\"              \n","\n","# Result CSV name\n","resultname = \"{}_T{}_results.csv\".format(name, recnum)\n","\n","#Result file address\n","resultfileadd = resultsDir+resultname\n","\n","print(resultfileadd)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/models/KMEANS_ftt_pmeans5_K350_R30000_F1500_model.pkl\n","/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/MegaDfs/KMEANS_ftt_pmeans5_K350_R30000_F1500_clustered_megadf.pkl\n","/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/results/KMEANS_ftt_pmeans5_K350_R30000_F1500_T2_results.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LGy8jsWQx0nS"},"source":["# Actual Code"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"II0jpbbTzD8M"},"source":["### imports and time"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wkS4KGOphHK4","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn import cluster, datasets\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","import seaborn as sns\n","import os, subprocess, sys\n","import datetime, time\n","import pickle\n","from scipy.spatial.distance import cosine \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlG-euRY8MJc","colab_type":"text"},"source":["## Load the megadfs"]},{"cell_type":"code","metadata":{"id":"LXoKaIBKTvjx","colab_type":"code","colab":{}},"source":["tempmdf = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/MegaDfs/Abstract_Abstract_KMEANS_use_use_K350_R30000_F512_clustered_megadf.pkl\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdZCDyZPUZbB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1589516543153,"user_tz":-330,"elapsed":1347,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"4ba3127a-c4dd-43c9-955f-e090601a92f1"},"source":["tempmdf.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Title</th>\n","      <th>Abstract</th>\n","      <th>Topic</th>\n","      <th>Title_Embedding</th>\n","      <th>Abstract_Embedding</th>\n","      <th>clusterlabel_title</th>\n","      <th>clusterlabel_abs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>26299</th>\n","      <td>http://arxiv.org/abs/1508.00761v1</td>\n","      <td>Recognition of Emotions using Kinects</td>\n","      <td>Psychological studies indicate that emotional ...</td>\n","      <td>psychology</td>\n","      <td>[0.053678866, -0.009632988, -0.07267857, 0.005...</td>\n","      <td>[0.020883536, -0.031098202, -0.066623844, 0.01...</td>\n","      <td>20</td>\n","      <td>252</td>\n","    </tr>\n","    <tr>\n","      <th>8393</th>\n","      <td>http://arxiv.org/abs/quant-ph/0604072v1</td>\n","      <td>Brief History of Quantum Cryptography: A Perso...</td>\n","      <td>Quantum cryptography is the only approach to p...</td>\n","      <td>cryptography</td>\n","      <td>[0.020271683, -0.036928527, -0.049796335, -0.0...</td>\n","      <td>[-0.046857614, -0.043386728, -0.048527252, -0....</td>\n","      <td>102</td>\n","      <td>178</td>\n","    </tr>\n","    <tr>\n","      <th>15272</th>\n","      <td>http://arxiv.org/abs/cond-mat/0409378v2</td>\n","      <td>Geographical threshold graphs with small-world...</td>\n","      <td>Many real networks are equipped with short dia...</td>\n","      <td>geography</td>\n","      <td>[0.011794036, -0.05427748, -0.012652266, -0.00...</td>\n","      <td>[-0.05856386, -0.04959274, -0.064852424, -0.00...</td>\n","      <td>164</td>\n","      <td>139</td>\n","    </tr>\n","    <tr>\n","      <th>11476</th>\n","      <td>http://arxiv.org/abs/0906.2611v2</td>\n","      <td>SEAMONSTER: A Demonstration Sensor Web Operati...</td>\n","      <td>A sensor web is a collection of heterogeneous ...</td>\n","      <td>education</td>\n","      <td>[-0.0042912737, 0.04556092, -0.057276912, -0.0...</td>\n","      <td>[-0.026967276, 0.05848821, -0.03232752, -0.059...</td>\n","      <td>169</td>\n","      <td>326</td>\n","    </tr>\n","    <tr>\n","      <th>19085</th>\n","      <td>http://arxiv.org/abs/0806.4827v3</td>\n","      <td>2-D color code quantum computation</td>\n","      <td>We describe in detail how to perform universal...</td>\n","      <td>logic</td>\n","      <td>[0.047185894, -0.06582154, -0.053225275, -0.00...</td>\n","      <td>[-0.059069965, -0.034812808, -0.010245204, -0....</td>\n","      <td>338</td>\n","      <td>280</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            id  ... clusterlabel_abs\n","26299        http://arxiv.org/abs/1508.00761v1  ...              252\n","8393   http://arxiv.org/abs/quant-ph/0604072v1  ...              178\n","15272  http://arxiv.org/abs/cond-mat/0409378v2  ...              139\n","11476         http://arxiv.org/abs/0906.2611v2  ...              326\n","19085         http://arxiv.org/abs/0806.4827v3  ...              280\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"0aSNIf-qUcXx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":179},"executionInfo":{"status":"ok","timestamp":1589516649147,"user_tz":-330,"elapsed":1538,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"8450311d-20a2-4b4d-c9e2-6198c89f96bf"},"source":["sampledf = tempmdf.sample(2, random_state=69)\n","sampledf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Title</th>\n","      <th>Abstract</th>\n","      <th>Topic</th>\n","      <th>Title_Embedding</th>\n","      <th>Abstract_Embedding</th>\n","      <th>clusterlabel_title</th>\n","      <th>clusterlabel_abs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>28944</th>\n","      <td>http://arxiv.org/abs/1611.01962v1</td>\n","      <td>High-Resolution Semantic Labeling with Convolu...</td>\n","      <td>Convolutional neural networks (CNNs) have rece...</td>\n","      <td>semantics</td>\n","      <td>[0.054926984, -0.03251063, -0.031229218, -0.05...</td>\n","      <td>[0.014008306, -0.05429825, -0.03890696, -0.062...</td>\n","      <td>93</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>518</th>\n","      <td>http://arxiv.org/abs/1309.6589v3</td>\n","      <td>Mind: An Archaeological Perspective</td>\n","      <td>What can relics of the past tell us about the ...</td>\n","      <td>archaeology</td>\n","      <td>[0.013196699, 0.03508813, 0.013127662, -0.0555...</td>\n","      <td>[-0.047361284, 0.040542364, 0.0077921017, -0.0...</td>\n","      <td>269</td>\n","      <td>331</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      id  ... clusterlabel_abs\n","28944  http://arxiv.org/abs/1611.01962v1  ...               11\n","518     http://arxiv.org/abs/1309.6589v3  ...              331\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"PDMWo1jm8Su0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1589516553629,"user_tz":-330,"elapsed":1884,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"d6ff546d-f808-43fb-e268-2a28dccf36d9"},"source":["model =\"/content/drive/My Drive/ColabOutput/arxiv_glove/Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_model_new.pkl\"   #@param {type: \"string\"}\n","\n","\n","megadf = \"/content/drive/My Drive/ColabOutput/arxiv_glove/Abstract_Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_clustered_megadf.pkl\"   #@param {type: \"string\"}\n","\n","#store names\n","\n","model_name = model\n","megadf_name = megadf\n","\n","#Load Megadf\n","print(\"Loading megadf {}\".format(megadf_name))\n","megadf = pickle.load(open(megadf_name, \"rb\"))\n","print(\"{} loading done!\".format(megadf_name))\n"," \n","#Load model\n","print(\"Loading model {}\".format(model_name))\n","model = pickle.load(open(model_name, \"rb\"))\n","print(\"{} loading done!\".format(model_name))\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading megadf /content/drive/My Drive/ColabOutput/arxiv_glove/Abstract_Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_clustered_megadf.pkl\n","/content/drive/My Drive/ColabOutput/arxiv_glove/Abstract_Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_clustered_megadf.pkl loading done!\n","Loading model /content/drive/My Drive/ColabOutput/arxiv_glove/Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_model_new.pkl\n","/content/drive/My Drive/ColabOutput/arxiv_glove/Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_model_new.pkl loading done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14jK2QEXVUaj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"ok","timestamp":1589516557196,"user_tz":-330,"elapsed":1478,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"4e629e8e-c053-4bb6-dd16-349108666272"},"source":["megadf.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Title</th>\n","      <th>Abstract</th>\n","      <th>Topic</th>\n","      <th>Title_Embedding</th>\n","      <th>Abstract_Embedding</th>\n","      <th>clusterlabel_title</th>\n","      <th>clusterlabel_abs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>21837</th>\n","      <td>http://arxiv.org/abs/1803.07682v1</td>\n","      <td>A Feature-Driven Active Framework for Ultrasou...</td>\n","      <td>A reliable Ultrasound (US)-to-US registration ...</td>\n","      <td>neurology</td>\n","      <td>[0.4475177777777778, 0.05546344444444446, 0.12...</td>\n","      <td>[0.23881284531516372, 0.0025751437923080575, 0...</td>\n","      <td>53</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2718</th>\n","      <td>http://arxiv.org/abs/0704.3853v1</td>\n","      <td>Fluorescence microscopy of single autofluoresc...</td>\n","      <td>In this paper we review the applicability of a...</td>\n","      <td>biology</td>\n","      <td>[0.2681670559083188, 0.12890394363986055, 0.38...</td>\n","      <td>[0.3078997253086889, 0.11462310730583875, 0.11...</td>\n","      <td>1</td>\n","      <td>134</td>\n","    </tr>\n","    <tr>\n","      <th>7341</th>\n","      <td>http://arxiv.org/abs/hep-th/0403052v2</td>\n","      <td>Entropy Bounds and Dark Energy</td>\n","      <td>Entropy bounds render quantum corrections to t...</td>\n","      <td>cosmology</td>\n","      <td>[0.18931425, 0.4385825, 0.11791142500000001, 0...</td>\n","      <td>[0.24607449733595432, 0.24399425671709019, 0.2...</td>\n","      <td>220</td>\n","      <td>342</td>\n","    </tr>\n","    <tr>\n","      <th>15716</th>\n","      <td>http://arxiv.org/abs/0710.4494v1</td>\n","      <td>Mean-value property on manifolds with minimal ...</td>\n","      <td>Let (M,g) be a non-compact and complete Rieman...</td>\n","      <td>harmonics</td>\n","      <td>[0.34303456522637193, 0.3023412675798373, 0.17...</td>\n","      <td>[0.3208850447266551, 0.22531258348331698, 0.07...</td>\n","      <td>285</td>\n","      <td>169</td>\n","    </tr>\n","    <tr>\n","      <th>9090</th>\n","      <td>http://arxiv.org/abs/1312.6321v2</td>\n","      <td>The virus of my virus is my friend: ecological...</td>\n","      <td>Virophages are viruses that rely on the replic...</td>\n","      <td>ecology</td>\n","      <td>[0.648300975857385, -0.1613049765602169, -0.01...</td>\n","      <td>[0.42954541646774863, -0.023605147673582048, 0...</td>\n","      <td>135</td>\n","      <td>117</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          id  ... clusterlabel_abs\n","21837      http://arxiv.org/abs/1803.07682v1  ...                4\n","2718        http://arxiv.org/abs/0704.3853v1  ...              134\n","7341   http://arxiv.org/abs/hep-th/0403052v2  ...              342\n","15716       http://arxiv.org/abs/0710.4494v1  ...              169\n","9090        http://arxiv.org/abs/1312.6321v2  ...              117\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"kl3CoHPZVstx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1589516561699,"user_tz":-330,"elapsed":1374,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"cd67ad47-4ad7-4535-8b79-b5cea7276672"},"source":["megadf.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['id', 'Title', 'Abstract', 'Topic', 'Title_Embedding',\n","       'Abstract_Embedding', 'clusterlabel_title', 'clusterlabel_abs'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"id":"OSmXrMTkg1GW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1589516563040,"user_tz":-330,"elapsed":1619,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"4f7ba386-4946-4faa-b6f3-af092ebe83d2"},"source":["megadf.iloc[28944]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["id                                     http://arxiv.org/abs/1404.3785v1\n","Title                 Reducing the Barrier to Entry of Complex Robot...\n","Abstract              Developing robot agnostic software frameworks ...\n","Topic                                                          robotics\n","Title_Embedding       [0.5503432657064702, 0.009375289497669328, 0.1...\n","Abstract_Embedding    [0.2876241740996425, -0.14520031104398326, 0.2...\n","clusterlabel_title                                                  333\n","clusterlabel_abs                                                    245\n","Name: 26846, dtype: object"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"code","metadata":{"id":"BR7Uht-5gWVp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589516567437,"user_tz":-330,"elapsed":1529,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"5ce2b7a6-3c13-4148-f306-35228f708e5e"},"source":["l = list(sampledf[\"id\"])\n","l"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['http://arxiv.org/abs/1611.01962v1', 'http://arxiv.org/abs/1309.6589v3']"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"code","metadata":{"id":"XVWnDW6Fjaff","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589516570111,"user_tz":-330,"elapsed":1504,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"fb924094-8f7c-4663-aa52-3644606e05dd"},"source":["megadf[\"id\"].iloc[0] in l"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"gAnYXYUPfb81","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"status":"ok","timestamp":1589516716939,"user_tz":-330,"elapsed":1462,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"b28ae91c-16c3-4c97-d0d8-b4e4f98d3a7d"},"source":["sampledf = megadf[megadf[\"id\"].isin(l) ]\n","sampledf = sampledf.iloc[:-1, :]\n","sampledf\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Title</th>\n","      <th>Abstract</th>\n","      <th>Topic</th>\n","      <th>Title_Embedding</th>\n","      <th>Abstract_Embedding</th>\n","      <th>clusterlabel_title</th>\n","      <th>clusterlabel_abs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>28498</th>\n","      <td>http://arxiv.org/abs/1611.01962v1</td>\n","      <td>High-Resolution Semantic Labeling with Convolu...</td>\n","      <td>Convolutional neural networks (CNNs) have rece...</td>\n","      <td>semantics</td>\n","      <td>[-0.04270571428571427, -0.08312285714285712, 0...</td>\n","      <td>[0.11632750997868145, -0.007022299927966316, 0...</td>\n","      <td>29</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>http://arxiv.org/abs/1309.6589v3</td>\n","      <td>Mind: An Archaeological Perspective</td>\n","      <td>What can relics of the past tell us about the ...</td>\n","      <td>archaeology</td>\n","      <td>[0.3902666666666667, 0.6073000000000001, -0.60...</td>\n","      <td>[0.24949665317062777, -0.007534590037428, -0.1...</td>\n","      <td>180</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      id  ... clusterlabel_abs\n","28498  http://arxiv.org/abs/1611.01962v1  ...                8\n","72      http://arxiv.org/abs/1309.6589v3  ...               11\n","\n","[2 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"gO3eDoyDgasD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589516724210,"user_tz":-330,"elapsed":1466,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"8b9d435b-df1b-488e-f7db-9f6d42d61e05"},"source":["predata = sampledf[\"Abstract_Embedding\"]\n","data = np.matrix(predata.to_list())\n","print(data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(2, 250)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mqo95BJ8fbJf","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mjaEme8V_g8u","colab_type":"text"},"source":["# Start Predicting"]},{"cell_type":"markdown","metadata":{"id":"zZpuugqKM6SD","colab_type":"text"},"source":["## Predict labels for the input abstracts"]},{"cell_type":"code","metadata":{"id":"lE8tUj-W_qFT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589516725817,"user_tz":-330,"elapsed":900,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"e0564470-a9ca-4898-903d-63b8747c0260"},"source":["pred_labels = model.predict(data, )\n","pred_labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 8, 11], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"markdown","metadata":{"id":"Zw849SyVNKjs","colab_type":"text"},"source":["### Store labels in the df"]},{"cell_type":"code","metadata":{"id":"H4-nJcOsAwe2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"status":"ok","timestamp":1589516730563,"user_tz":-330,"elapsed":1384,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"d8aaddfe-adf7-400d-b2d0-a12e060d98d4"},"source":["sampledf[\"Abstract_pred_labels\"] = pred_labels\n","sampledf"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Title</th>\n","      <th>Abstract</th>\n","      <th>Topic</th>\n","      <th>Title_Embedding</th>\n","      <th>Abstract_Embedding</th>\n","      <th>clusterlabel_title</th>\n","      <th>clusterlabel_abs</th>\n","      <th>Abstract_pred_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>28498</th>\n","      <td>http://arxiv.org/abs/1611.01962v1</td>\n","      <td>High-Resolution Semantic Labeling with Convolu...</td>\n","      <td>Convolutional neural networks (CNNs) have rece...</td>\n","      <td>semantics</td>\n","      <td>[-0.04270571428571427, -0.08312285714285712, 0...</td>\n","      <td>[0.11632750997868145, -0.007022299927966316, 0...</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>http://arxiv.org/abs/1309.6589v3</td>\n","      <td>Mind: An Archaeological Perspective</td>\n","      <td>What can relics of the past tell us about the ...</td>\n","      <td>archaeology</td>\n","      <td>[0.3902666666666667, 0.6073000000000001, -0.60...</td>\n","      <td>[0.24949665317062777, -0.007534590037428, -0.1...</td>\n","      <td>180</td>\n","      <td>11</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      id  ... Abstract_pred_labels\n","28498  http://arxiv.org/abs/1611.01962v1  ...                    8\n","72      http://arxiv.org/abs/1309.6589v3  ...                   11\n","\n","[2 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"PWlqI5hUOGEY","colab_type":"text"},"source":["## Create Result Dictionary"]},{"cell_type":"code","metadata":{"id":"CFBr4YCmL1Fq","colab_type":"code","colab":{}},"source":["result_dict = {\n","    \"id\": [],\n","    \"Title\": [],\n","    \"Abstract\": [],\n","    \"Rank_1_id\": [],\n","    \"Rank_1_Title\": [],\n","    \"Rank_1_Abstract\": [],\n","    \"Rank_1_Topic\":[],\n","    \"Rank_1_cosdis\": [],\n","    \"Rank_2_id\": [],\n","    \"Rank_2_Title\": [],\n","    \"Rank_2_Abstract\": [],\n","    \"Rank_2_Topic\":[],\n","    \"Rank_2_cosdis\": [],\n","    \"Rank_3_id\": [],\n","    \"Rank_3_Title\": [],\n","    \"Rank_3_Abstract\": [],\n","    \"Rank_3_Topic\":[],\n","    \"Rank_3_cosdis\": [],\n","    \"Rank_4_id\": [],\n","    \"Rank_4_Title\": [],\n","    \"Rank_4_Abstract\": [],\n","    \"Rank_4_Topic\":[],\n","    \"Rank_4_cosdis\": [],\n","    \"Rank_5_id\": [],\n","    \"Rank_5_Title\": [],\n","    \"Rank_5_Abstract\": [],\n","    \"Rank_5_Topic\":[],\n","    \"Rank_5_cosdis\": [],\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjoOgL1kOfpQ","colab_type":"text"},"source":["## Cosine Distance function\n","\n"]},{"cell_type":"code","metadata":{"id":"pyPxexjVOAxK","colab_type":"code","colab":{}},"source":["def cos_dis(x, y):    \n","    return cosine(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LGk0L75UOt36","colab_type":"text"},"source":["## Filling Result dictionary"]},{"cell_type":"code","metadata":{"id":"bd3WPc2IC3Gx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"status":"ok","timestamp":1589517241363,"user_tz":-330,"elapsed":1804,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"19e028a8-ee14-406e-b879-988351191a86"},"source":["for i in range(int(sampledf.shape[0])):\n","  newabs_clustter_df = pd.DataFrame()\n","#   paper_data = db.getpapers([int(megadf[\"id\"].iloc[i]),])[0]\n","  result_dict[\"id\"].append(sampledf[\"id\"].iloc[i])\n","  result_dict[\"Title\"].append(sampledf[\"Title\"].iloc[i])\n","  result_dict[\"Abstract\"].append(sampledf[\"Abstract\"].iloc[i])\n","  print(result_dict)\n","  print(\"@@@\")\n","  newabs_label = int(sampledf[\"Abstract_pred_labels\"].iloc[i])\n","  newabs_clustter_df = megadf[megadf[\"clusterlabel_abs\"] == newabs_label]\n","  print(\"###\")\n","  newabs_clustter_df[\"Dist2New\"] = newabs_clustter_df[\"Abstract_Embedding\"].apply(cos_dis, args = (sampledf[\"Abstract_Embedding\"].iloc[i],))\n","  print(\"$$$$\")\n","  newabs_clustter_df = newabs_clustter_df.sort_values(by = \"Dist2New\")\n","  \n","  related_papers = newabs_clustter_df.head()\n","  print(\"%%%\")\n","  for j in range(1, related_papers.shape[0]+1):\n","    result_dict[\"Rank_{}_id\".format(j)].append(related_papers[\"id\"].iloc[j-1])\n","    result_dict[\"Rank_{}_Title\".format(j)].append(related_papers[\"Title\"].iloc[j-1])\n","    result_dict[\"Rank_{}_Abstract\".format(j)].append(related_papers[\"Abstract\"].iloc[j-1])\n","    result_dict[\"Rank_{}_Topic\".format(j)].append(related_papers[\"Topic\"].iloc[j-1])\n","    result_dict[\"Rank_{}_cosdis\".format(j)].append(related_papers[\"Dist2New\"].iloc[j-1])\n","    print(\"^^^^\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'id': ['http://arxiv.org/abs/1611.01962v1'], 'Title': ['High-Resolution Semantic Labeling with Convolutional Neural Networks'], 'Abstract': ['Convolutional neural networks (CNNs) have received increasing attention overthe last few years. They were initially conceived for image categorization,i.e., the problem of assigning a semantic label to an entire input image.  In this paper we address the problem of dense semantic labeling, whichconsists in assigning a semantic label to every pixel in an image. Since thisrequires a high spatial accuracy to determine where labels are assigned,categorization CNNs, intended to be highly robust to local deformations, arenot directly applicable.  By adapting categorization networks, many semantic labeling CNNs have beenrecently proposed. Our first contribution is an in-depth analysis of thesearchitectures. We establish the desired properties of an ideal semanticlabeling CNN, and assess how those methods stand with regard to theseproperties. We observe that even though they provide competitive results, theseCNNs often underexploit properties of semantic labeling that could lead to moreeffective and efficient architectures.  Out of these observations, we then derive a CNN framework specificallyadapted to the semantic labeling problem. In addition to learning features atdifferent resolutions, it learns how to combine these features. By integratinglocal and global information in an efficient and flexible manner, itoutperforms previous techniques. We evaluate the proposed framework and compareit with state-of-the-art architectures on public benchmarks of high-resolutionaerial image labeling.'], 'Rank_1_id': [], 'Rank_1_Title': [], 'Rank_1_Abstract': [], 'Rank_1_Topic': [], 'Rank_1_cosdis': [], 'Rank_2_id': [], 'Rank_2_Title': [], 'Rank_2_Abstract': [], 'Rank_2_Topic': [], 'Rank_2_cosdis': [], 'Rank_3_id': [], 'Rank_3_Title': [], 'Rank_3_Abstract': [], 'Rank_3_Topic': [], 'Rank_3_cosdis': [], 'Rank_4_id': [], 'Rank_4_Title': [], 'Rank_4_Abstract': [], 'Rank_4_Topic': [], 'Rank_4_cosdis': [], 'Rank_5_id': [], 'Rank_5_Title': [], 'Rank_5_Abstract': [], 'Rank_5_Topic': [], 'Rank_5_cosdis': []}\n","@@@\n","###\n","$$$$\n","%%%\n","^^^^\n","^^^^\n","^^^^\n","^^^^\n","^^^^\n","{'id': ['http://arxiv.org/abs/1611.01962v1', 'http://arxiv.org/abs/1309.6589v3'], 'Title': ['High-Resolution Semantic Labeling with Convolutional Neural Networks', 'Mind: An Archaeological Perspective'], 'Abstract': ['Convolutional neural networks (CNNs) have received increasing attention overthe last few years. They were initially conceived for image categorization,i.e., the problem of assigning a semantic label to an entire input image.  In this paper we address the problem of dense semantic labeling, whichconsists in assigning a semantic label to every pixel in an image. Since thisrequires a high spatial accuracy to determine where labels are assigned,categorization CNNs, intended to be highly robust to local deformations, arenot directly applicable.  By adapting categorization networks, many semantic labeling CNNs have beenrecently proposed. Our first contribution is an in-depth analysis of thesearchitectures. We establish the desired properties of an ideal semanticlabeling CNN, and assess how those methods stand with regard to theseproperties. We observe that even though they provide competitive results, theseCNNs often underexploit properties of semantic labeling that could lead to moreeffective and efficient architectures.  Out of these observations, we then derive a CNN framework specificallyadapted to the semantic labeling problem. In addition to learning features atdifferent resolutions, it learns how to combine these features. By integratinglocal and global information in an efficient and flexible manner, itoutperforms previous techniques. We evaluate the proposed framework and compareit with state-of-the-art architectures on public benchmarks of high-resolutionaerial image labeling.', 'What can relics of the past tell us about the thoughts and beliefs of thepeople who invented and used them? Recent collaborations at the frontier ofarchaeology, anthropology, and cognitive science are culminating in speculativebut nevertheless increasingly sophisticated efforts to unravel how modern humancognition came about. By considering objects within their archaeologicalcontext, we have begun to piece together something of the way of life of peoplewho inhabited particular locales, which in turn reflects their underlyingthought processes.'], 'Rank_1_id': ['http://arxiv.org/abs/1611.01962v1'], 'Rank_1_Title': ['High-Resolution Semantic Labeling with Convolutional Neural Networks'], 'Rank_1_Abstract': ['Convolutional neural networks (CNNs) have received increasing attention overthe last few years. They were initially conceived for image categorization,i.e., the problem of assigning a semantic label to an entire input image.  In this paper we address the problem of dense semantic labeling, whichconsists in assigning a semantic label to every pixel in an image. Since thisrequires a high spatial accuracy to determine where labels are assigned,categorization CNNs, intended to be highly robust to local deformations, arenot directly applicable.  By adapting categorization networks, many semantic labeling CNNs have beenrecently proposed. Our first contribution is an in-depth analysis of thesearchitectures. We establish the desired properties of an ideal semanticlabeling CNN, and assess how those methods stand with regard to theseproperties. We observe that even though they provide competitive results, theseCNNs often underexploit properties of semantic labeling that could lead to moreeffective and efficient architectures.  Out of these observations, we then derive a CNN framework specificallyadapted to the semantic labeling problem. In addition to learning features atdifferent resolutions, it learns how to combine these features. By integratinglocal and global information in an efficient and flexible manner, itoutperforms previous techniques. We evaluate the proposed framework and compareit with state-of-the-art architectures on public benchmarks of high-resolutionaerial image labeling.'], 'Rank_1_Topic': ['semantics'], 'Rank_1_cosdis': [0.0], 'Rank_2_id': ['http://arxiv.org/abs/1610.01944v3'], 'Rank_2_Title': ['PetroSurf3D - A Dataset for high-resolution 3D Surface Segmentation'], 'Rank_2_Abstract': ['The development of powerful 3D scanning hardware and reconstructionalgorithms has strongly promoted the generation of 3D surface reconstructionsin different domains. An area of special interest for such 3D reconstructionsis the cultural heritage domain, where surface reconstructions are generated todigitally preserve historical artifacts. While reconstruction quality nowadaysis sufficient in many cases, the robust analysis (e.g. segmentation, matching,and classification) of reconstructed 3D data is still an open topic. In thispaper, we target the automatic and interactive segmentation of high-resolution3D surface reconstructions from the archaeological domain. To foster researchin this field, we introduce a fully annotated and publicly availablelarge-scale 3D surface dataset including high-resolution meshes, depth maps andpoint clouds as a novel benchmark dataset to the community. We provide baselineresults for our existing random forest-based approach and for the first timeinvestigate segmentation with convolutional neural networks (CNNs) on the data.Results show that both approaches have complementary strengths and weaknessesand that the provided dataset represents a challenge for future research.'], 'Rank_2_Topic': ['archaeology'], 'Rank_2_cosdis': [0.01573769196218322], 'Rank_3_id': ['http://arxiv.org/abs/1906.01290v1'], 'Rank_3_Title': ['Relational Reasoning using Prior Knowledge for Visual Captioning'], 'Rank_3_Abstract': ['Exploiting relationships among objects has achieved remarkable progress ininterpreting images or videos by natural language. Most existing methods resortto first detecting objects and their relationships, and then generating textualdescriptions, which heavily depends on pre-trained detectors and leads toperformance drop when facing problems of heavy occlusion, tiny-size objects andlong-tail in object detection. In addition, the separate procedure of detectingand captioning results in semantic inconsistency between the pre-definedobject/relation categories and the target lexical words. We exploit prior humancommonsense knowledge for reasoning relationships between objects without anypre-trained detectors and reaching semantic coherency within one image or videoin captioning. The prior knowledge (e.g., in the form of knowledge graph)provides commonsense semantic correlation and constraint between objects thatare not explicit in the image and video, serving as useful guidance to buildsemantic graph for sentence generation. Particularly, we present a jointreasoning method that incorporates 1) commonsense reasoning for embedding imageor video regions into semantic space to build semantic graph and 2) relationalreasoning for encoding semantic graph to generate sentences. Extensiveexperiments on the MS-COCO image captioning benchmark and the MSVD videocaptioning benchmark validate the superiority of our method on leveraging priorcommonsense knowledge to enhance relational reasoning for visual captioning.'], 'Rank_3_Topic': ['semantics'], 'Rank_3_cosdis': [0.01720484776304787], 'Rank_4_id': ['http://arxiv.org/abs/1803.06067v1'], 'Rank_4_Title': ['Dynamic-structured Semantic Propagation Network'], 'Rank_4_Abstract': ['Semantic concept hierarchy is still under-explored for semantic segmentationdue to the inefficiency and complicated optimization of incorporatingstructural inference into dense prediction. This lack of modeling semanticcorrelations also makes prior works must tune highly-specified models for eachtask due to the label discrepancy across datasets. It severely limits thegeneralization capability of segmentation models for open set conceptvocabulary and annotation utilization. In this paper, we propose aDynamic-Structured Semantic Propagation Network (DSSPN) that builds a semanticneuron graph by explicitly incorporating the semantic concept hierarchy intonetwork construction. Each neuron represents the instantiated module forrecognizing a specific type of entity such as a super-class (e.g. food) or aspecific concept (e.g. pizza). During training, DSSPN performs thedynamic-structured neuron computation graph by only activating a sub-graph ofneurons for each image in a principled way. A dense semantic-enhanced neuralblock is proposed to propagate the learned knowledge of all ancestor neuronsinto each fine-grained child neuron for feature evolving. Another merit of suchsemantic explainable structure is the ability of learning a unified modelconcurrently on diverse datasets by selectively activating different neuronsub-graphs for each annotation at each step. Extensive experiments on fourpublic semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape andMapillary) demonstrate the superiority of our DSSPN over state-of-the-artsegmentation models. Moreoever, we demonstrate a universal segmentation modelthat is jointly trained on diverse datasets can surpass the performance of thecommon fine-tuning scheme for exploiting multiple domain knowledge.'], 'Rank_4_Topic': ['semantics'], 'Rank_4_cosdis': [0.018573322061708586], 'Rank_5_id': ['http://arxiv.org/abs/1912.13282v1'], 'Rank_5_Title': ['Medusa: A C++ Library for solving PDEs using Strong Form Mesh-Free\\n  methods'], 'Rank_5_Abstract': ['Medusa, a novel library for implementation of strong form mesh-free methods,is described. We identify and present common parts and patterns among many suchmethods reported in the literature, such as node positioning, stencil selectionand stencil weight computation. Many different algorithms exist for each partand the possible combinations offer a plethora of possibilities forimprovements of solution procedures that are far from fully understood. As aconsequence there are still many unanswered questions in mesh-free communityresulting in vivid ongoing research in the field. Medusa implements the coremesh-free elements as independent blocks, which offers users great flexibilityin experimenting with the method they are developing, as well as easilycomparing it with other existing methods. The paper describes the chosenabstractions and their usage, illustrates aspects of the philosophy and design,offers some executions time benchmarks and demonstrates the application of thelibrary on cases from linear elasticity and fluid flow in irregular 2D and 3Ddomains.'], 'Rank_5_Topic': ['philosophy'], 'Rank_5_cosdis': [0.019061241473773416]}\n","@@@\n","###\n","$$$$\n","%%%\n","^^^^\n","^^^^\n","^^^^\n","^^^^\n","^^^^\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"URsOH03PPfET","colab_type":"text"},"source":["## Convert Result Dictionary to Dataframe\n","### Save the dataframe as csv in Results dir"]},{"cell_type":"code","metadata":{"id":"ut495pOagm--","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1589517241364,"user_tz":-330,"elapsed":1791,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"8cc3f28a-00e3-4496-960c-65fa4c6918b8"},"source":["print(result_dict)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'id': ['http://arxiv.org/abs/1611.01962v1', 'http://arxiv.org/abs/1309.6589v3'], 'Title': ['High-Resolution Semantic Labeling with Convolutional Neural Networks', 'Mind: An Archaeological Perspective'], 'Abstract': ['Convolutional neural networks (CNNs) have received increasing attention overthe last few years. They were initially conceived for image categorization,i.e., the problem of assigning a semantic label to an entire input image.  In this paper we address the problem of dense semantic labeling, whichconsists in assigning a semantic label to every pixel in an image. Since thisrequires a high spatial accuracy to determine where labels are assigned,categorization CNNs, intended to be highly robust to local deformations, arenot directly applicable.  By adapting categorization networks, many semantic labeling CNNs have beenrecently proposed. Our first contribution is an in-depth analysis of thesearchitectures. We establish the desired properties of an ideal semanticlabeling CNN, and assess how those methods stand with regard to theseproperties. We observe that even though they provide competitive results, theseCNNs often underexploit properties of semantic labeling that could lead to moreeffective and efficient architectures.  Out of these observations, we then derive a CNN framework specificallyadapted to the semantic labeling problem. In addition to learning features atdifferent resolutions, it learns how to combine these features. By integratinglocal and global information in an efficient and flexible manner, itoutperforms previous techniques. We evaluate the proposed framework and compareit with state-of-the-art architectures on public benchmarks of high-resolutionaerial image labeling.', 'What can relics of the past tell us about the thoughts and beliefs of thepeople who invented and used them? Recent collaborations at the frontier ofarchaeology, anthropology, and cognitive science are culminating in speculativebut nevertheless increasingly sophisticated efforts to unravel how modern humancognition came about. By considering objects within their archaeologicalcontext, we have begun to piece together something of the way of life of peoplewho inhabited particular locales, which in turn reflects their underlyingthought processes.'], 'Rank_1_id': ['http://arxiv.org/abs/1611.01962v1', 'http://arxiv.org/abs/1309.6589v3'], 'Rank_1_Title': ['High-Resolution Semantic Labeling with Convolutional Neural Networks', 'Mind: An Archaeological Perspective'], 'Rank_1_Abstract': ['Convolutional neural networks (CNNs) have received increasing attention overthe last few years. They were initially conceived for image categorization,i.e., the problem of assigning a semantic label to an entire input image.  In this paper we address the problem of dense semantic labeling, whichconsists in assigning a semantic label to every pixel in an image. Since thisrequires a high spatial accuracy to determine where labels are assigned,categorization CNNs, intended to be highly robust to local deformations, arenot directly applicable.  By adapting categorization networks, many semantic labeling CNNs have beenrecently proposed. Our first contribution is an in-depth analysis of thesearchitectures. We establish the desired properties of an ideal semanticlabeling CNN, and assess how those methods stand with regard to theseproperties. We observe that even though they provide competitive results, theseCNNs often underexploit properties of semantic labeling that could lead to moreeffective and efficient architectures.  Out of these observations, we then derive a CNN framework specificallyadapted to the semantic labeling problem. In addition to learning features atdifferent resolutions, it learns how to combine these features. By integratinglocal and global information in an efficient and flexible manner, itoutperforms previous techniques. We evaluate the proposed framework and compareit with state-of-the-art architectures on public benchmarks of high-resolutionaerial image labeling.', 'What can relics of the past tell us about the thoughts and beliefs of thepeople who invented and used them? Recent collaborations at the frontier ofarchaeology, anthropology, and cognitive science are culminating in speculativebut nevertheless increasingly sophisticated efforts to unravel how modern humancognition came about. By considering objects within their archaeologicalcontext, we have begun to piece together something of the way of life of peoplewho inhabited particular locales, which in turn reflects their underlyingthought processes.'], 'Rank_1_Topic': ['semantics', 'anthropology'], 'Rank_1_cosdis': [0.0, 0.0], 'Rank_2_id': ['http://arxiv.org/abs/1610.01944v3', 'http://arxiv.org/abs/1309.6589v3'], 'Rank_2_Title': ['PetroSurf3D - A Dataset for high-resolution 3D Surface Segmentation', 'Mind: An Archaeological Perspective'], 'Rank_2_Abstract': ['The development of powerful 3D scanning hardware and reconstructionalgorithms has strongly promoted the generation of 3D surface reconstructionsin different domains. An area of special interest for such 3D reconstructionsis the cultural heritage domain, where surface reconstructions are generated todigitally preserve historical artifacts. While reconstruction quality nowadaysis sufficient in many cases, the robust analysis (e.g. segmentation, matching,and classification) of reconstructed 3D data is still an open topic. In thispaper, we target the automatic and interactive segmentation of high-resolution3D surface reconstructions from the archaeological domain. To foster researchin this field, we introduce a fully annotated and publicly availablelarge-scale 3D surface dataset including high-resolution meshes, depth maps andpoint clouds as a novel benchmark dataset to the community. We provide baselineresults for our existing random forest-based approach and for the first timeinvestigate segmentation with convolutional neural networks (CNNs) on the data.Results show that both approaches have complementary strengths and weaknessesand that the provided dataset represents a challenge for future research.', 'What can relics of the past tell us about the thoughts and beliefs of thepeople who invented and used them? Recent collaborations at the frontier ofarchaeology, anthropology, and cognitive science are culminating in speculativebut nevertheless increasingly sophisticated efforts to unravel how modern humancognition came about. By considering objects within their archaeologicalcontext, we have begun to piece together something of the way of life of peoplewho inhabited particular locales, which in turn reflects their underlyingthought processes.'], 'Rank_2_Topic': ['archaeology', 'archaeology'], 'Rank_2_cosdis': [0.01573769196218322, 0.0], 'Rank_3_id': ['http://arxiv.org/abs/1906.01290v1', 'http://arxiv.org/abs/1912.00914v2'], 'Rank_3_Title': ['Relational Reasoning using Prior Knowledge for Visual Captioning', 'A Changing Dichotomy: The Conception of the \"Macroscopic\" and\\n  \"Microscopic\" Worlds in the History of Physics'], 'Rank_3_Abstract': ['Exploiting relationships among objects has achieved remarkable progress ininterpreting images or videos by natural language. Most existing methods resortto first detecting objects and their relationships, and then generating textualdescriptions, which heavily depends on pre-trained detectors and leads toperformance drop when facing problems of heavy occlusion, tiny-size objects andlong-tail in object detection. In addition, the separate procedure of detectingand captioning results in semantic inconsistency between the pre-definedobject/relation categories and the target lexical words. We exploit prior humancommonsense knowledge for reasoning relationships between objects without anypre-trained detectors and reaching semantic coherency within one image or videoin captioning. The prior knowledge (e.g., in the form of knowledge graph)provides commonsense semantic correlation and constraint between objects thatare not explicit in the image and video, serving as useful guidance to buildsemantic graph for sentence generation. Particularly, we present a jointreasoning method that incorporates 1) commonsense reasoning for embedding imageor video regions into semantic space to build semantic graph and 2) relationalreasoning for encoding semantic graph to generate sentences. Extensiveexperiments on the MS-COCO image captioning benchmark and the MSVD videocaptioning benchmark validate the superiority of our method on leveraging priorcommonsense knowledge to enhance relational reasoning for visual captioning.', 'This short essay traces the conceptual history of micro- and macroscopicityin the context of physical science. By focusing on three distinct episodesspanning five centuries, we show the scientific and philosophical meanings ofthis antonym pair, despite never being far from \"the small\" and \"the large,\"have been evolving as the frontier of science advances. We analyze theintellectual and material impetus for these movements, and conclude that thisconceptual history reflects the changing interaction between the natural worldand humankind.'], 'Rank_3_Topic': ['semantics', 'history'], 'Rank_3_cosdis': [0.01720484776304787, 0.025871226036779604], 'Rank_4_id': ['http://arxiv.org/abs/1803.06067v1', 'http://arxiv.org/abs/0906.0328v1'], 'Rank_4_Title': ['Dynamic-structured Semantic Propagation Network', 'Rivisiting Token/Bucket Algorithms in New Applications'], 'Rank_4_Abstract': ['Semantic concept hierarchy is still under-explored for semantic segmentationdue to the inefficiency and complicated optimization of incorporatingstructural inference into dense prediction. This lack of modeling semanticcorrelations also makes prior works must tune highly-specified models for eachtask due to the label discrepancy across datasets. It severely limits thegeneralization capability of segmentation models for open set conceptvocabulary and annotation utilization. In this paper, we propose aDynamic-Structured Semantic Propagation Network (DSSPN) that builds a semanticneuron graph by explicitly incorporating the semantic concept hierarchy intonetwork construction. Each neuron represents the instantiated module forrecognizing a specific type of entity such as a super-class (e.g. food) or aspecific concept (e.g. pizza). During training, DSSPN performs thedynamic-structured neuron computation graph by only activating a sub-graph ofneurons for each image in a principled way. A dense semantic-enhanced neuralblock is proposed to propagate the learned knowledge of all ancestor neuronsinto each fine-grained child neuron for feature evolving. Another merit of suchsemantic explainable structure is the ability of learning a unified modelconcurrently on diverse datasets by selectively activating different neuronsub-graphs for each annotation at each step. Extensive experiments on fourpublic semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape andMapillary) demonstrate the superiority of our DSSPN over state-of-the-artsegmentation models. Moreoever, we demonstrate a universal segmentation modelthat is jointly trained on diverse datasets can surpass the performance of thecommon fine-tuning scheme for exploiting multiple domain knowledge.', 'We consider a somehow peculiar Token/Bucket problem which at first sightlooks confusing and difficult to solve. The winning approach to solve theproblem consists in going back to the simple and traditional methods to solvecomputer science problems like the one taught to us by Knuth. Somehow the maintrick is to be able to specify clearly what needs to be achieved, and then thesolution, even if complex, appears almost by itself.'], 'Rank_4_Topic': ['semantics', 'computer+science'], 'Rank_4_cosdis': [0.018573322061708586, 0.028156842947243632], 'Rank_5_id': ['http://arxiv.org/abs/1912.13282v1', 'http://arxiv.org/abs/1511.08033v1'], 'Rank_5_Title': ['Medusa: A C++ Library for solving PDEs using Strong Form Mesh-Free\\n  methods', 'On Mathematical Symbols in China'], 'Rank_5_Abstract': ['Medusa, a novel library for implementation of strong form mesh-free methods,is described. We identify and present common parts and patterns among many suchmethods reported in the literature, such as node positioning, stencil selectionand stencil weight computation. Many different algorithms exist for each partand the possible combinations offer a plethora of possibilities forimprovements of solution procedures that are far from fully understood. As aconsequence there are still many unanswered questions in mesh-free communityresulting in vivid ongoing research in the field. Medusa implements the coremesh-free elements as independent blocks, which offers users great flexibilityin experimenting with the method they are developing, as well as easilycomparing it with other existing methods. The paper describes the chosenabstractions and their usage, illustrates aspects of the philosophy and design,offers some executions time benchmarks and demonstrates the application of thelibrary on cases from linear elasticity and fluid flow in irregular 2D and 3Ddomains.', 'When studying the history of mathematical symbols, one finds that thedevelopment of mathematical symbols in China is a significant piece of Chinesehistory; however, between the beginning of mathematics and modern daymathematics in China, there exists a long blank period. Let us focus on thedevelopment of Chinese mathematical symbols, and find out the significance oftheir origin, evolution, rise and fall within Chinese mathematics.'], 'Rank_5_Topic': ['philosophy', 'mathematics'], 'Rank_5_cosdis': [0.019061241473773416, 0.02985876599965953]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ypA-ynZZcrrl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"status":"ok","timestamp":1589517241365,"user_tz":-330,"elapsed":1778,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"9c76e733-fe61-4091-efd8-9b37c58cec20"},"source":["resultsdf = pd.DataFrame(result_dict)\n","resultsdf.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Title</th>\n","      <th>Abstract</th>\n","      <th>Rank_1_id</th>\n","      <th>Rank_1_Title</th>\n","      <th>Rank_1_Abstract</th>\n","      <th>Rank_1_Topic</th>\n","      <th>Rank_1_cosdis</th>\n","      <th>Rank_2_id</th>\n","      <th>Rank_2_Title</th>\n","      <th>Rank_2_Abstract</th>\n","      <th>Rank_2_Topic</th>\n","      <th>Rank_2_cosdis</th>\n","      <th>Rank_3_id</th>\n","      <th>Rank_3_Title</th>\n","      <th>Rank_3_Abstract</th>\n","      <th>Rank_3_Topic</th>\n","      <th>Rank_3_cosdis</th>\n","      <th>Rank_4_id</th>\n","      <th>Rank_4_Title</th>\n","      <th>Rank_4_Abstract</th>\n","      <th>Rank_4_Topic</th>\n","      <th>Rank_4_cosdis</th>\n","      <th>Rank_5_id</th>\n","      <th>Rank_5_Title</th>\n","      <th>Rank_5_Abstract</th>\n","      <th>Rank_5_Topic</th>\n","      <th>Rank_5_cosdis</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>http://arxiv.org/abs/1611.01962v1</td>\n","      <td>High-Resolution Semantic Labeling with Convolu...</td>\n","      <td>Convolutional neural networks (CNNs) have rece...</td>\n","      <td>http://arxiv.org/abs/1611.01962v1</td>\n","      <td>High-Resolution Semantic Labeling with Convolu...</td>\n","      <td>Convolutional neural networks (CNNs) have rece...</td>\n","      <td>semantics</td>\n","      <td>0.0</td>\n","      <td>http://arxiv.org/abs/1610.01944v3</td>\n","      <td>PetroSurf3D - A Dataset for high-resolution 3D...</td>\n","      <td>The development of powerful 3D scanning hardwa...</td>\n","      <td>archaeology</td>\n","      <td>0.015738</td>\n","      <td>http://arxiv.org/abs/1906.01290v1</td>\n","      <td>Relational Reasoning using Prior Knowledge for...</td>\n","      <td>Exploiting relationships among objects has ach...</td>\n","      <td>semantics</td>\n","      <td>0.017205</td>\n","      <td>http://arxiv.org/abs/1803.06067v1</td>\n","      <td>Dynamic-structured Semantic Propagation Network</td>\n","      <td>Semantic concept hierarchy is still under-expl...</td>\n","      <td>semantics</td>\n","      <td>0.018573</td>\n","      <td>http://arxiv.org/abs/1912.13282v1</td>\n","      <td>Medusa: A C++ Library for solving PDEs using S...</td>\n","      <td>Medusa, a novel library for implementation of ...</td>\n","      <td>philosophy</td>\n","      <td>0.019061</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>http://arxiv.org/abs/1309.6589v3</td>\n","      <td>Mind: An Archaeological Perspective</td>\n","      <td>What can relics of the past tell us about the ...</td>\n","      <td>http://arxiv.org/abs/1309.6589v3</td>\n","      <td>Mind: An Archaeological Perspective</td>\n","      <td>What can relics of the past tell us about the ...</td>\n","      <td>anthropology</td>\n","      <td>0.0</td>\n","      <td>http://arxiv.org/abs/1309.6589v3</td>\n","      <td>Mind: An Archaeological Perspective</td>\n","      <td>What can relics of the past tell us about the ...</td>\n","      <td>archaeology</td>\n","      <td>0.000000</td>\n","      <td>http://arxiv.org/abs/1912.00914v2</td>\n","      <td>A Changing Dichotomy: The Conception of the \"M...</td>\n","      <td>This short essay traces the conceptual history...</td>\n","      <td>history</td>\n","      <td>0.025871</td>\n","      <td>http://arxiv.org/abs/0906.0328v1</td>\n","      <td>Rivisiting Token/Bucket Algorithms in New Appl...</td>\n","      <td>We consider a somehow peculiar Token/Bucket pr...</td>\n","      <td>computer+science</td>\n","      <td>0.028157</td>\n","      <td>http://arxiv.org/abs/1511.08033v1</td>\n","      <td>On Mathematical Symbols in China</td>\n","      <td>When studying the history of mathematical symb...</td>\n","      <td>mathematics</td>\n","      <td>0.029859</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  id  ... Rank_5_cosdis\n","0  http://arxiv.org/abs/1611.01962v1  ...      0.019061\n","1   http://arxiv.org/abs/1309.6589v3  ...      0.029859\n","\n","[2 rows x 28 columns]"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"code","metadata":{"id":"nHmfAfYJdXVR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1589517241366,"user_tz":-330,"elapsed":1769,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"3073672f-9a5e-4cb3-daad-8d31f32c7031"},"source":["resultfileadd = \"/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/results/new/\"+ model_name.split(\"/\")[-1][:-4] + \"_results\" + \".csv\"\n","resultfileadd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Colab Notebooks/BTechProject_Venkatesh/results/new/Abstract_KMEANS_glove_pmeans5_K350_R30000_F250_model_new_results.csv'"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"code","metadata":{"id":"LAg23Jvdj9qG","colab_type":"code","colab":{}},"source":["resultsdf.to_csv(resultfileadd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPcW8Totigcd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600154312958,"user_tz":-330,"elapsed":3380,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"ab594223-c7b9-438a-a28c-ad9fc9ef1fdd"},"source":["! pip list"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Package                       Version        \n","----------------------------- ---------------\n","absl-py                       0.10.0         \n","alabaster                     0.7.12         \n","albumentations                0.1.12         \n","altair                        4.1.0          \n","argon2-cffi                   20.1.0         \n","asgiref                       3.2.10         \n","astor                         0.8.1          \n","astropy                       4.0.1.post1    \n","astunparse                    1.6.3          \n","async-generator               1.10           \n","atari-py                      0.2.6          \n","atomicwrites                  1.4.0          \n","attrs                         20.2.0         \n","audioread                     2.1.8          \n","autograd                      1.3            \n","Babel                         2.8.0          \n","backcall                      0.2.0          \n","beautifulsoup4                4.6.3          \n","bleach                        3.1.5          \n","blis                          0.4.1          \n","bokeh                         2.1.1          \n","boto                          2.49.0         \n","boto3                         1.14.59        \n","botocore                      1.17.59        \n","Bottleneck                    1.3.2          \n","branca                        0.4.1          \n","bs4                           0.0.1          \n","CacheControl                  0.12.6         \n","cachetools                    4.1.1          \n","catalogue                     1.0.0          \n","certifi                       2020.6.20      \n","cffi                          1.14.2         \n","chainer                       7.4.0          \n","chardet                       3.0.4          \n","click                         7.1.2          \n","cloudpickle                   1.3.0          \n","cmake                         3.12.0         \n","cmdstanpy                     0.9.5          \n","colorlover                    0.3.0          \n","community                     1.0.0b1        \n","contextlib2                   0.5.5          \n","convertdate                   2.2.2          \n","coverage                      3.7.1          \n","coveralls                     0.5            \n","crcmod                        1.7            \n","cufflinks                     0.17.3         \n","cvxopt                        1.2.5          \n","cvxpy                         1.0.31         \n","cycler                        0.10.0         \n","cymem                         2.0.3          \n","Cython                        0.29.21        \n","daft                          0.0.4          \n","dask                          2.12.0         \n","dataclasses                   0.7            \n","datascience                   0.10.6         \n","debugpy                       1.0.0rc2       \n","decorator                     4.4.2          \n","defusedxml                    0.6.0          \n","descartes                     1.1.0          \n","dill                          0.3.2          \n","distributed                   1.25.3         \n","Django                        3.1.1          \n","dlib                          19.18.0        \n","dm-tree                       0.1.5          \n","docopt                        0.6.2          \n","docutils                      0.15.2         \n","dopamine-rl                   1.0.5          \n","earthengine-api               0.1.234        \n","easydict                      1.9            \n","ecos                          2.0.7.post1    \n","editdistance                  0.5.3          \n","en-core-web-sm                2.2.5          \n","entrypoints                   0.3            \n","ephem                         3.7.7.1        \n","et-xmlfile                    1.0.1          \n","fa2                           0.3.5          \n","fancyimpute                   0.4.3          \n","fastai                        1.0.61         \n","fastdtw                       0.3.4          \n","fastprogress                  1.0.0          \n","fastrlock                     0.5            \n","fbprophet                     0.7.1          \n","feather-format                0.4.1          \n","filelock                      3.0.12         \n","firebase-admin                4.1.0          \n","fix-yahoo-finance             0.0.22         \n","Flask                         1.1.2          \n","folium                        0.8.3          \n","future                        0.16.0         \n","gast                          0.3.3          \n","GDAL                          2.2.2          \n","gdown                         3.6.4          \n","gensim                        3.6.0          \n","geographiclib                 1.50           \n","geopy                         1.17.0         \n","gin-config                    0.3.0          \n","glob2                         0.7            \n","google                        2.0.3          \n","google-api-core               1.16.0         \n","google-api-python-client      1.7.12         \n","google-auth                   1.17.2         \n","google-auth-httplib2          0.0.4          \n","google-auth-oauthlib          0.4.1          \n","google-cloud-bigquery         1.21.0         \n","google-cloud-core             1.0.3          \n","google-cloud-datastore        1.8.0          \n","google-cloud-firestore        1.7.0          \n","google-cloud-language         1.2.0          \n","google-cloud-storage          1.18.1         \n","google-cloud-translate        1.5.0          \n","google-colab                  1.0.0          \n","google-pasta                  0.2.0          \n","google-resumable-media        0.4.1          \n","googleapis-common-protos      1.52.0         \n","googledrivedownloader         0.4            \n","graphviz                      0.10.1         \n","grpcio                        1.32.0         \n","gspread                       3.0.1          \n","gspread-dataframe             3.0.8          \n","gym                           0.17.2         \n","h5py                          2.10.0         \n","HeapDict                      1.0.1          \n","holidays                      0.10.3         \n","holoviews                     1.13.3         \n","html5lib                      1.0.1          \n","httpimport                    0.5.18         \n","httplib2                      0.17.4         \n","httplib2shim                  0.0.3          \n","humanize                      0.5.1          \n","hyperopt                      0.1.2          \n","ideep4py                      2.0.0.post3    \n","idna                          2.10           \n","image                         1.5.32         \n","imageio                       2.4.1          \n","imagesize                     1.2.0          \n","imbalanced-learn              0.4.3          \n","imblearn                      0.0            \n","imgaug                        0.2.9          \n","importlib-metadata            1.7.0          \n","imutils                       0.5.3          \n","inflect                       2.1.0          \n","iniconfig                     1.0.1          \n","intel-openmp                  2020.0.133     \n","intervaltree                  2.1.0          \n","ipykernel                     4.10.1         \n","ipython                       5.5.0          \n","ipython-genutils              0.2.0          \n","ipython-sql                   0.3.9          \n","ipywidgets                    7.5.1          \n","itsdangerous                  1.1.0          \n","jax                           0.1.75         \n","jaxlib                        0.1.52         \n","jdcal                         1.4.1          \n","jedi                          0.17.2         \n","jieba                         0.42.1         \n","Jinja2                        2.11.2         \n","jmespath                      0.10.0         \n","joblib                        0.16.0         \n","jpeg4py                       0.1.4          \n","jsonschema                    2.6.0          \n","jupyter                       1.0.0          \n","jupyter-client                5.3.5          \n","jupyter-console               5.2.0          \n","jupyter-core                  4.6.3          \n","jupyterlab-pygments           0.1.1          \n","kaggle                        1.5.8          \n","kapre                         0.1.3.1        \n","Keras                         2.4.3          \n","Keras-Preprocessing           1.1.2          \n","keras-vis                     0.4.1          \n","kiwisolver                    1.2.0          \n","knnimpute                     0.1.0          \n","korean-lunar-calendar         0.2.1          \n","librosa                       0.6.3          \n","lightgbm                      2.2.3          \n","llvmlite                      0.31.0         \n","lmdb                          0.99           \n","lucid                         0.3.8          \n","LunarCalendar                 0.0.9          \n","lxml                          4.2.6          \n","Markdown                      3.2.2          \n","MarkupSafe                    1.1.1          \n","matplotlib                    3.2.2          \n","matplotlib-venn               0.11.5         \n","missingno                     0.4.2          \n","mistune                       0.8.4          \n","mizani                        0.6.0          \n","mkl                           2019.0         \n","mlxtend                       0.14.0         \n","more-itertools                8.5.0          \n","moviepy                       0.2.3.5        \n","mpmath                        1.1.0          \n","msgpack                       1.0.0          \n","multiprocess                  0.70.10        \n","multitasking                  0.0.9          \n","murmurhash                    1.0.2          \n","music21                       5.5.0          \n","natsort                       5.5.0          \n","nbclient                      0.5.0          \n","nbconvert                     5.6.1          \n","nbformat                      5.0.7          \n","nest-asyncio                  1.4.0          \n","networkx                      2.5            \n","nibabel                       3.0.2          \n","nltk                          3.2.5          \n","notebook                      5.3.1          \n","np-utils                      0.5.12.1       \n","numba                         0.48.0         \n","numexpr                       2.7.1          \n","numpy                         1.18.5         \n","nvidia-ml-py3                 7.352.0        \n","oauth2client                  4.1.3          \n","oauthlib                      3.1.0          \n","okgrade                       0.4.3          \n","opencv-contrib-python         4.1.2.30       \n","opencv-python                 4.1.2.30       \n","openpyxl                      2.5.9          \n","opt-einsum                    3.3.0          \n","osqp                          0.6.1          \n","packaging                     20.4           \n","palettable                    3.3.0          \n","pandas                        1.0.5          \n","pandas-datareader             0.8.1          \n","pandas-gbq                    0.11.0         \n","pandas-profiling              1.4.1          \n","pandocfilters                 1.4.2          \n","panel                         0.9.7          \n","param                         1.9.3          \n","parso                         0.7.1          \n","pathlib                       1.0.1          \n","patsy                         0.5.1          \n","pexpect                       4.8.0          \n","pickleshare                   0.7.5          \n","Pillow                        7.0.0          \n","pip                           19.3.1         \n","pip-tools                     4.5.1          \n","plac                          1.1.3          \n","plotly                        4.4.1          \n","plotnine                      0.6.0          \n","pluggy                        0.7.1          \n","portpicker                    1.3.1          \n","prefetch-generator            1.0.1          \n","preshed                       3.0.2          \n","prettytable                   0.7.2          \n","progressbar2                  3.38.0         \n","prometheus-client             0.8.0          \n","promise                       2.3            \n","prompt-toolkit                1.0.18         \n","protobuf                      3.12.4         \n","psutil                        5.4.8          \n","psycopg2                      2.7.6.1        \n","ptyprocess                    0.6.0          \n","py                            1.9.0          \n","pyarrow                       0.14.1         \n","pyasn1                        0.4.8          \n","pyasn1-modules                0.2.8          \n","pycocotools                   2.0.2          \n","pycparser                     2.20           \n","pyct                          0.4.7          \n","pydata-google-auth            1.1.0          \n","pydot                         1.3.0          \n","pydot-ng                      2.0.0          \n","pydotplus                     2.0.2          \n","PyDrive                       1.3.1          \n","pyemd                         0.5.1          \n","pyglet                        1.5.0          \n","Pygments                      2.6.1          \n","pygobject                     3.26.1         \n","pymc3                         3.7            \n","PyMeeus                       0.3.7          \n","pymongo                       3.11.0         \n","pymystem3                     0.2.0          \n","PyOpenGL                      3.1.5          \n","pyparsing                     2.4.7          \n","pyrsistent                    0.16.0         \n","pysndfile                     1.3.8          \n","PySocks                       1.7.1          \n","pystan                        2.19.1.1       \n","pytest                        3.6.4          \n","python-apt                    1.6.5+ubuntu0.3\n","python-chess                  0.23.11        \n","python-dateutil               2.8.1          \n","python-louvain                0.14           \n","python-slugify                4.0.1          \n","python-utils                  2.4.0          \n","pytz                          2018.9         \n","pyviz-comms                   0.7.6          \n","PyWavelets                    1.1.1          \n","PyYAML                        3.13           \n","pyzmq                         19.0.2         \n","qtconsole                     4.7.7          \n","QtPy                          1.9.0          \n","regex                         2019.12.20     \n","requests                      2.23.0         \n","requests-oauthlib             1.3.0          \n","resampy                       0.2.2          \n","retrying                      1.3.3          \n","rpy2                          3.2.7          \n","rsa                           4.6            \n","s3transfer                    0.3.3          \n","scikit-image                  0.16.2         \n","scikit-learn                  0.22.2.post1   \n","scipy                         1.4.1          \n","screen-resolution-extra       0.0.0          \n","scs                           2.1.2          \n","seaborn                       0.10.1         \n","Send2Trash                    1.5.0          \n","setuptools                    50.3.0         \n","setuptools-git                1.2            \n","Shapely                       1.7.1          \n","simplegeneric                 0.8.1          \n","six                           1.15.0         \n","sklearn                       0.0            \n","sklearn-pandas                1.8.0          \n","slugify                       0.0.1          \n","smart-open                    2.1.1          \n","snowballstemmer               2.0.0          \n","sortedcontainers              2.2.2          \n","spacy                         2.2.4          \n","Sphinx                        1.8.5          \n","sphinxcontrib-serializinghtml 1.1.4          \n","sphinxcontrib-websupport      1.2.4          \n","SQLAlchemy                    1.3.19         \n","sqlparse                      0.3.1          \n","srsly                         1.0.2          \n","statsmodels                   0.10.2         \n","sympy                         1.1.1          \n","tables                        3.4.4          \n","tabulate                      0.8.7          \n","tblib                         1.7.0          \n","tensorboard                   2.3.0          \n","tensorboard-plugin-wit        1.7.0          \n","tensorboardcolab              0.0.22         \n","tensorflow                    2.3.0          \n","tensorflow-addons             0.8.3          \n","tensorflow-datasets           2.1.0          \n","tensorflow-estimator          2.3.0          \n","tensorflow-gcs-config         2.3.0          \n","tensorflow-hub                0.9.0          \n","tensorflow-metadata           0.24.0         \n","tensorflow-privacy            0.2.2          \n","tensorflow-probability        0.11.0         \n","termcolor                     1.1.0          \n","terminado                     0.8.3          \n","testpath                      0.4.4          \n","text-unidecode                1.3            \n","textblob                      0.15.3         \n","textgenrnn                    1.4.1          \n","Theano                        1.0.5          \n","thinc                         7.4.0          \n","tifffile                      2020.9.3       \n","toml                          0.10.1         \n","toolz                         0.10.0         \n","torch                         1.6.0+cu101    \n","torchsummary                  1.5.1          \n","torchtext                     0.3.1          \n","torchvision                   0.7.0+cu101    \n","tornado                       5.1.1          \n","tqdm                          4.41.1         \n","traitlets                     4.3.3          \n","tweepy                        3.6.0          \n","typeguard                     2.7.1          \n","typing-extensions             3.7.4.3        \n","tzlocal                       1.5.1          \n","umap-learn                    0.4.6          \n","uritemplate                   3.0.1          \n","urllib3                       1.24.3         \n","vega-datasets                 0.8.0          \n","wasabi                        0.8.0          \n","wcwidth                       0.2.5          \n","webencodings                  0.5.1          \n","Werkzeug                      1.0.1          \n","wheel                         0.35.1         \n","widgetsnbextension            3.5.1          \n","wordcloud                     1.5.0          \n","wrapt                         1.12.1         \n","xarray                        0.15.1         \n","xgboost                       0.90           \n","xkit                          0.0.0          \n","xlrd                          1.1.0          \n","xlwt                          1.3.0          \n","yellowbrick                   0.9.1          \n","zict                          2.0.0          \n","zipp                          3.1.0          \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ByzwmzkuoRNP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600154422747,"user_tz":-330,"elapsed":2062,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}}},"source":["import sklearn\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"56WMQLmqor-G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600154440060,"user_tz":-330,"elapsed":1169,"user":{"displayName":"Venkatesh Yelnoorkar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWlk-wDS3XJuyxZbIp3HhQaCdEZDTfkr4LitbbIg=s64","userId":"05835981926124646102"}},"outputId":"cb76f658-ea34-496f-e8b2-4891e1531e87"},"source":["sklearn.__version__"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0.22.2.post1'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"x57yUwd6owyc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}